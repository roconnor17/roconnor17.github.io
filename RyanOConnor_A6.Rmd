---
title: "Assignment 6"
author: "RyanOConnor"
date: "11/02/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r}
library(tidyverse)
library(leaflet)
library(censusapi)
library(sf)
library(mapview)
library(tigris)
library(readxl)
library(survey)
library(dplyr)

Sys.setenv(CENSUS_KEY="cccdf64b447748daaddebe5f9aeaec92fc4b744c")
```

#**PART 1:**

In Figure 1 below, we see the Public Use Microdata Areas (PUMAs) for San Francisco County, CA mapped. In this example, we can see some strange geographic anomalies in the borders for these PUMAs. First, when looking at North Beach and Chinatown PUMA in the Northeast portion of the city, we see that the shape also encompasses land are across the Bay in Alameda in the southwest corner of the airfield, as well as on Angel Island near Fort McDowell. When examining the OpenStreetMap layer in the leaflet map output, it becomes clear why these anomalies occur. Both shapes terminate on one end along the San Francisco County line, which to me indicates that the automated model that the Census Bureau used to generate their PUMA shapefiles clipped them at county lines. Because of the quirk of where the San Francisco County line terminates, this meant that small slivers of land area became attributed to what I suspect the Census Bureau would agree is the wrong PUMA. A third geographic anomaly can be seen as part of the Richmond District. Several miles out to see West of the Bay is the Southeast Farralon Island State Marine Conservation Area. As the OpenStreetMap layer shows, this area is jurisdictionally part of San Francisco City and County, and therefor became attributed within the PUMA of Northwest San Francisco County. Fortunately, all three of these anomalous areas are uninhabited; one is a park, one a decommissioned airfield, one a wildlife preserve. So for the purposes of analysis, they should not affect any outputs when investigating PUMS data. 

```{r}
pums_2019_1yr <- getCensus(
  name = "acs/acs1/pums",
  vintage = 2019,
  region = "public use microdata area:*", 
  regionin = "state:06",
  vars = c(
    "SERIALNO",
    "SPORDER",
    "YBL",
    "BLD",
    "MV",
    "NP",
    "HHL",
    "HINCP",
    "TEN",
    "AGEP",
    "HUPAC",
    "PUMA"
  )
)

ca_pumas <-
  pumas("CA", cb = T, progress_bar = F)

SF_county_name <-
"San Francisco"

SF_county <-
  counties("CA", cb = T, progress_bar = F) %>%
  filter(NAME %in% SF_county_name)

SF_PUMA <-
  ca_pumas %>% 
  st_centroid() %>% 
  .[SF_county, ] %>% 
  st_drop_geometry() %>% 
  left_join(ca_pumas %>% select(GEOID10)) %>% 
  st_as_sf()

leaflet() %>%
  addTiles() %>% 
  addPolygons(
    data = SF_PUMA,
    fillColor = "Blue",
    color = "white",
    opacity = 0.5,
    fillOpacity = 0.3,
    weight = 2,
    label = SF_PUMA$NAME10,
    highlightOptions = highlightOptions(
      weight = 2,
      opacity = 1
    )
  )
```

**Figure 1. A map of San Francisco County Public Use Microdata Areas (PUMAs)


Below are the outputs of a generalized linear model that is designed to predict the likelihood that a given house qualifies for a lead testing kit, which is defined as households that a) make less than $90,000 annually, b) have at least one child below the age of 6, and c) live in a home built prior to 1960. The parameters of the model are the type of building (units in structure), the tenure of the residents, when it was moved into, and the geography of the home (PUMA, see Figure 1). The model is designed to predict, based on these parameters, whether a house will qualify for a lead testing kit. Based on the summary of outputs below, we can see that five factors hold statistical significance in the prediction; two tenure factors and 3 geographic PUMAs. 

```{r}
SF_pums <-
  pums_2019_1yr %>% 
  mutate(
    PUMA = str_pad(public_use_microdata_area,5,"left","0")
  ) %>% 
  filter(PUMA %in% SF_PUMA$PUMACE10)

SF_pums_clean <-
  SF_pums %>% 
  mutate(YBL = as.numeric(YBL),
         HINCP = as.numeric(HINCP),
         BLD = as.numeric(BLD),
         TEN = as.numeric(TEN),
         MV = as.numeric(MV),
         PUMA = as.numeric(PUMA),
         AGEP = as.numeric(AGEP)) %>%
  filter(SF_pums$YBL %in% 1:3) %>%
  group_by(SERIALNO) %>%
  arrange(AGEP) %>%
  summarize_all(first)

SF_pums_leadrisk <- SF_pums_clean %>% 
  mutate(leadrisk =
           ifelse(
             (HINCP < 90000) &
               (AGEP %in% 0:5),
             1,
             0
           ))

SF_pums_factor <- SF_pums_leadrisk %>%
  mutate(
    building = BLD %>%
      factor(
        levels = SF_pums_leadrisk$BLD %>%
          unique() %>%
          as.numeric() %>%
          sort()
      ),
    tenure = TEN %>%
      factor(
        levels = SF_pums_leadrisk$TEN %>%
          unique() %>%
          as.numeric() %>%
          sort()
      ),
    move = MV %>%
      factor(
        levels = SF_pums_leadrisk$MV %>%
          unique() %>%
          as.numeric() %>%
          sort()
      ),
    puma1 = PUMA %>%
      factor(
        levels = SF_pums_leadrisk$PUMA %>%
          unique() %>%
          as.numeric() %>%
          sort()
      ),
    leadrisk = as.numeric(leadrisk)
  )

```

Summary output of the logit model with the factors of Building, Tenure, Move, and PUMA:

```{r}
SF_pums_logit <- glm(
  leadrisk ~ building + tenure + move + puma1,
  family = quasibinomial(),
  data = SF_pums_factor
)

summary(SF_pums_logit)
```


Below is the predicted probability of a randomly selected row that a home has a household income below $90,000 per year _and_ has at least one child below the age of six (i.e. their "leadrisk" is TRUE:

```{r}
predict(SF_pums_logit, sample_n(SF_pums_factor,1), type = "response")
```

#**PART 2:**


Below we present a 2x2 matrix representing True Positives, True Negatives, False Positives, and False Negatives. In the upper right and lower left, we can see the true positives and negatives, respectively. In the upper left and lower right, we see the Type II (False Negative) and Type I error (False Positive), respectively. In total __the model correctly predicted 97.3% of the outcomes, with 4 true positives and 2200 true negatives.__ The 4 homes that were true positives all had significant similarities. They were all single-family detached homes, they all rented, and they had all moved in to their homes within the past 9 years.

Interestingly, however, the model predicted over 6x more false positives (27) than true positives. Based on the prediction, this program would mail out 31 test kits per ~2200 homes (scaling up for the full population of San Francisco), however __only about 13% of mailed lead test kits would successfully reach qualifying households__ (4 true positives out of 31 true predictions). Meanwhile, the model predicted 33 homes as not receiving test kits, when in fact they do qualify. __Only about 10% of homes that do in fact qualify would actually receive a test kit, meaning only about 10% of at-risk children would be reached by a program following this model__ (4 true positives out of 37 true outcomes). Although this analysis does not account for replicate weights, we may still make assumptions for scaling up to the full populations, though they may hold limited statistical relevance. The PUMS data is a representative sample of approximately 10% of the population, therefore we must make some assumptions that the true quantity of all of our categories is simple 10x greater than the values presented in this analysis. Therefore, _the model would suggest that we should mail out 310 test kits ((4 + 27)* 10), and we should expect 40 of them to successfully reach at-risk homes._ This would leave 330 at-risk homes that would not receive a test kit, and _about 90% of at-risk would be left out of the program._

This model clearly lacks strong predictive capacity in terms of finding true positives, however it does rule out true negatives very effectively. The biggest issue, I believe, in the development of this model is that it targets a very small sub population based on variable factors that, by nature of the PUMS data, are only broken down into relatively large buckets (BLD, TEN, MV). Improving this model is challenging given the lack of immediately relevant information at the address level (income and children). The first thing that I would do is attempt to refine the model based on the factors that actually provided statistical significance; Tenure and PUMA geography. All other factors did not have significance in the model, and most had Pr(>|t|) values that were near-1. Therefore, we can reason that the majority of the model parameters do not actually predict significantly.  Once the model was improved, I would use it to target local school districts that may contain higher proportions of at-risk families and work with them on more targeted distribution, allowing the school staff to determine what local families or communities were more likely to be at-risk. Similarly to our discussion on how to most effectively distribute CalEnviroScreen funding, I believe that having local knowledge and on-the-ground collaboration is a critical piece in successfully distributing this kind of a resource. The school district may or may not have access to children counts (though I suspect they do) or income, however they will have a much greater understanding of which addresses have those specific risk factors than this rough model does, relying solely on other variables. 

```{r}
Full_predict <- predict(SF_pums_logit, SF_pums_factor, type = "response")

SF_pums_logit_predicted <- cbind(SF_pums_factor, Full_predict)
```

```{r}
summary_2x2 <-
  SF_pums_logit_predicted %>% 
  mutate(
    leadrisk = ifelse(
      leadrisk == 1, 
      "Lead Risk", 
      "No Lead Risk"
    )
  ) %>% 
  pull(leadrisk) %>% 
  table(Full_predict > 0.1)

summary_2x2
#Top Right = True Positive
#Bottom Left = True Negative
#Top Left = False Positive
#Bottom Right = False Negative
```

#**Part 3**

Below, the steps from the second half of Part 1 (model summary output) and Part 2 (2x2 matrix of predictions and errors) are repeated for the data set with replicate weights applied

We repeat the analysis for the 2x2 matrix representing True Positives, True Negatives, False Positives, and False Negatives. In the upper right and lower left, we can see the true positives and negatives, respectively. With the replicate weights accounted for, __the model correctly predicted 98.1% of the outcomes, with 2 true positives and 2220 true negatives.__ This represents a slight increase in overall success as compared to the un-weighted model, which we will see below is mostly accounted for in the reduction of false positive errors. 

Based on the weighted prediction, this program would mail out 9 test kits per ~2200 homes (scaling up for the full population of San Francisco), however __only about 22% of mailed lead test kits would successfully reach qualifying households__ (4 true positives out of 31 true predictions). This is an increase over the 13% that would successfully reach homes in the un-weighted model. Meanwhile, the model predicted 35 homes as not receiving test kits, when in fact they do qualify. In this case, __only about 5.4% of homes that do in fact qualify would actually receive a test kit, meaning only about 5.4% of at-risk children would be reached by a program following this model__ (2 true positives out of 37 true outcomes). Now that replicate weights are accounted for, a more appropriate scale up to the full population can be conducted, assuming that the sample is 10% of the full poopulation. _The model would suggest that we should mail out 90 test kits ((2+7)* 10), and we should expect 20 of them to successfully reach at-risk homes._ This would leave 70 at-risk homes that would not receive a test kit, and _about 68% of at-risk would be left out of the program._ While this result is more efficient in terms of the number of kits sent out, it remains a rough model, lacking predictive capacity to adequately cover the at-risk population. 

```{r}
pums_hca_2019_1yr <- read_csv("psam_h06.csv")

weights <- pums_hca_2019_1yr %>%
  filter(PUMA %in% SF_PUMA$PUMACE10) %>%
  select(
    SERIALNO,
    starts_with("WGTP")
  )
weighted_leadrisk <- left_join(
  weights,
  SF_pums_leadrisk,
  by = "SERIALNO"
) %>%
  filter_all(
    all_vars(!is.na(.))
  ) %>%
  mutate(
    leadrisk = as.numeric(leadrisk),
    BLD = factor(BLD),
    TEN = factor(TEN),
    MV = factor(MV),
    PUMA = factor(PUMA)
  )
  
design <- svrepdesign(
  data = weighted_leadrisk,
  type = "ACS",
  repweights = weighted_leadrisk[ ,3:82],
  weights = ~as.numeric(WGTP)
)

weighted_model <- svyglm(
  leadrisk ~ BLD + TEN + MV + PUMA,
  family = gaussian(),
  design = design
)
summary(weighted_model)
```

```{r}
predict(weighted_model, sample_n(weighted_leadrisk,1), type = "response")
```

```{r}
Weights_predict <- predict(weighted_model, weighted_leadrisk, type = "response")

Weighted_predicted <- cbind(weighted_leadrisk, Weights_predict)
```

```{r}
summary_2x2_weighted <-
  Weighted_predicted %>% 
  mutate(
    leadrisk = ifelse(
      leadrisk == 1, 
      "Lead Risk", 
      "No Lead Risk"
    )
  ) %>% 
  pull(leadrisk) %>% 
  table(Weights_predict >= 0.1)

summary_2x2_weighted
#Top Right = True Positive
#Bottom Left = True Negative
#Top Left = False Positive
#Bottom Right = False Negative
```
